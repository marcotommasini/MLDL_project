{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"loss.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SN574AV6TjWo"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable"]},{"cell_type":"code","source":["def flatten(tensor):\n","    \"\"\"Flattens a given tensor such that the channel axis is first.\n","    The shapes are transformed as follows:\n","       (N, C, D, H, W) -> (C, N * D * H * W)\n","    \"\"\"\n","    C = tensor.size(1)\n","    # new axis order\n","    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n","    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n","    transposed = tensor.permute(axis_order)\n","    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n","    return transposed.contiguous().view(C, -1)"],"metadata":{"id":"J7vEKkdETpht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class DiceLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.epsilon = 1e-5\n","\n","    def forward(self, output, target):\n","        assert output.size() == target.size(), \"'input' and 'target' must have the same shape\"\n","        output = F.softmax(output, dim=1)\n","        output = flatten(output)\n","        target = flatten(target)\n","        # intersect = (output * target).sum(-1).sum() + self.epsilon\n","        # denominator = ((output + target).sum(-1)).sum() + self.epsilon\n","\n","        intersect = (output * target).sum(-1)\n","        denominator = (output + target).sum(-1)\n","        dice = intersect / denominator\n","        dice = torch.mean(dice)\n","        return 1 - dice\n","        # return 1 - 2. * intersect / denominator"],"metadata":{"id":"XkDHMrvxTrOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CrossEntropyLoss(nn.Module):\n","  def __init__(self, background_label_value = 255):\n","    super().__init__()\n","    self.background = background_label_value\n","  \n","  def forward(self, prediction, target, weight = None):\n","    \n","    batch_size, number_classes, height, width = prediction.size()\n","    target_mask = (target >= 0) * (target != self.background)\n","    target = target[target_mask]\n","\n","    if not target.data.dim():\n","      return Variable(torch.zeros(1))\n","\n","    prediction = prediction.transpose(1, 2).transpose(2, 3).contiguous()\n","    prediction = prediction[target_mask.view(batch_size, height, width, 1).repeat(1, 1, 1, number_classes)].view(-1, number_classes)\n","\n","    loss = F.cross_entropy(prediction, target, weight=weight)\n","    return loss\n","    \n"],"metadata":{"id":"0pTyJB5BT7Gz"},"execution_count":null,"outputs":[]}]}